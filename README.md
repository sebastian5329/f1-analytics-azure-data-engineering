# ğŸï¸ F1 Historical Performance Analytics Pipeline

## ğŸ“Œ Overview  

This project implements a production-style data engineering platform on Azure to process and analyze historical Formula 1 data.

Built using:

- Azure Data Factory (ADF)
- Azure Databricks (PySpark, Spark SQL)
- Delta Lake
- Unity Catalog
- Azure Data Lake Storage Gen2

The solution follows a **Medallion Architecture (Bronze â†’ Silver â†’ Gold)** and supports incremental, scalable, and idempotent data processing.

The platform generates season-level driver and constructor rankings and enables standardized cross-era performance analysis.

---

## ğŸ¯ Business Objective  

Formula 1 scoring systems have changed across decades, making direct comparison of raw points unreliable.

This platform creates a standardized analytical foundation to answer questions such as:

- Who was the best driver in a given season?
- Which constructor dominated a specific year?
- Who accumulated the highest standardized performance across eras?

Rather than directly computing â€œbest of all time,â€ the system builds clean, incremental, analytics-ready datasets that enable reliable performance evaluation.

---

## ğŸ—ï¸ Architecture  

### ğŸ¥‰ Bronze Layer  
Raw data ingestion from ADLS.

### ğŸ¥ˆ Silver Layer  
- Schema enforcement using StructType  
- Column standardization (snake_case)  
- Flattening nested JSON structures  
- Metadata enrichment (`ingestion_date`, `file_date`, `data_source`)  
- Incremental upserts using Delta Lake  

### ğŸ¥‡ Gold Layer  
Business-ready analytical tables for rankings and performance analysis.

---

## ğŸ”„ Incremental Data Ingestion Strategy  

- File-based ingestion (file name contains date)
- Parameterized notebooks (`file_date`)
- Only new partitions processed
- Idempotent pipeline design
- Safe reprocessing supported

### â±ï¸ Tumbling Window Trigger (168 Hours)

ADF Tumbling Window Trigger configured with a 168-hour (7-day) interval.

Reason:
- F1 races occur on weekends
- Weekly scheduling aligns ingestion with race cycles
- Maintains non-overlapping, stateful execution

Benefits:
- Supports backfill and reprocessing
- Prevents duplicate loads
- Ensures predictable incremental processing

---

## ğŸ” Delta Lake Upsert Strategy  

Reusable PySpark utility function used for incremental merges:

- Implemented using `MERGE INTO`
- Avoids duplication
- Supports late-arriving data
- Maintains ACID compliance
- Enables dynamic partition overwrite

---

## ğŸ¥ˆ Silver Layer Tables  

Curated Delta tables:

- drivers  
- constructors  
- circuits  
- races  
- results  
- lap_times  
- pit_stops  
- qualifying  

All tables are incrementally maintained and partition-aware.

---

## ğŸ¥‡ Gold Layer â€“ Analytical Models  

### ğŸ Race Results (Enriched Fact Table)

Created by joining:

- drivers  
- constructors  
- circuits  
- races  
- results  

Provides:
- Driver details  
- Constructor details  
- Circuit information  
- Race metadata  
- Performance metrics  

This serves as the core analytical dataset.

---

### ğŸ§® Calculated Race Results (Standardized Scoring)

Problem:
F1 scoring systems vary across eras.

Solution:
Implemented a standardized scoring model:
11- position (for top 10 finishers)

This:
- Standardizes scoring for top 10 finishers
- Enables fair cross-era comparison
- Supports historical aggregation analysis

---

### ğŸ† Driver Standings  

Season-wise rankings generated by:

- Grouping by `race_year` and driver  
- Aggregating total points  
- Calculating wins (`position = 1`)  
- Ranking within each season  

Enables:
- Best driver per season analysis  
- Historical performance comparison  

---

### ğŸ—ï¸ Constructor Standings  

Similar aggregation logic applied at constructor level:

- Total season points  
- Total wins  
- Seasonal ranking  

Supports team dominance and performance trend analysis.

---

## âš™ï¸ Orchestration  

- Parameter-driven ADF pipelines  
- Modular Databricks notebooks  
- Master pipeline orchestration  
- Incremental window-based execution  
- End-to-end data lineage  

---

## ğŸš€ Key Engineering Highlights  

- Medallion architecture implementation  
- Incremental file-based ingestion  
- Delta Lake MERGE strategy  
- Idempotent data pipelines  
- Partition-aware processing  
- Reusable PySpark utility functions  
- Business-aligned scheduling  
- Analytical data modeling  

---

## ğŸ› ï¸ Tech Stack  

- Azure Data Factory  
- Azure Databricks  
- PySpark
- Spark SQL
- Delta Lake
- Unity Catalog
- Azure Data Lake Storage Gen2
- Access Connector

---

## ğŸ“Œ What This Project Demonstrates  

- Production-style Azure data engineering  
- Distributed data processing with PySpark  
- Incremental pipeline design  
- Delta Lake ACID upserts  
- Analytical modeling for reporting use cases  
- Business-aligned orchestration strategy  

---

This project reflects real-world data engineering practices focused on scalability, reliability, and analytics readiness.
